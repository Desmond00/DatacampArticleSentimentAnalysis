commentedBy,commentMessage,upvotes,commentDate
Karina Delcheva,"Hi! I'm non-academically trained in data analysis and loved your article. It's very useful and practically oriented. However, can you please give me a definition of ""informativeness"" as that's a term I have never encountered before?",0,23/02/2018 05:09 PM
tamireiny,"Hi, I have implemented the  Least Confidence,  Margin Sampling  and  Entropy Sampling  on the probability matrix
given from Logistic regression classifier. When I tried it on MNIST dataset (10 labels).
Seems that I get better results when selecting the sample to query randomly, then from selecting those examples based on any of the listed methods.
When analyzing the results I see that selected training labels are not uniform.
Any idea how to get better results using any of those methods?
Thanks!",3,16/05/2018 03:49 PM
Pradip Gupta,"Hi, ",4,25/07/2018 12:54 PM
Rebekah Waterbury,"Thank you for the tutorial.  It was helpful.  However my calculations for entropy for the instance d1 was .4457 and for d2 was 1.119.  In this case it would end up directing the process down the same path, that d2 would be more informative and therefore as per your example be queried. ",1,24/10/2018 12:55 AM
Hark Mark,"Thank you very much for well explained tutorial. I appreciate it. I am struggling to implement active learning in R, no packages are available for such learning have been found by me. A tutorial or course on R implementation of active learning would really be helpful. ",1,14/02/2019 02:35 PM
Will Stamatis,"This is wonderful! Minute detail--in the Steam-Based example, you say ""...repeat with the next IMAGINE"" instead of ""...next IMAGE.""",1,27/03/2019 03:15 AM
Keerthi Attygalle,If you don't mind sharing the code with me?,1,08/04/2019 11:36 PM
Jae Duk Seo,This is a great post,1,13/05/2019 10:19 AM
