{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will attempt to apply another type of text-data preprocessing viz. TF-IDF based vectorization. We will also try a simple neural network model to see if we can improve the baseline score. \n",
    "\n",
    "We could have gone for sequence models by incorporating word embeddings but the corpus that we are dealing with is not very large and experiments say that in these cases n-gram models perform better. Another reason behind not using word embeddings is embedding relationships are learned in dense space, and this happens best over many samples (i.e. a very very large corpora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our previous experiments, we serialized the shorter versions of the train and test sets. Let's load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_frame = pd.read_csv('./GoodData/final_data_extended_labeledPhase1 - final_data_extended_labeledPhase2.csv')\n",
    "# new_test_frame = pd.read_csv('new_test_frame.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_train_frame.drop(new_train_frame[new_train_frame['label'].isna()==True].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>articleName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for this tutorial , it is simple to ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Python List Comprehension Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Python List Comprehension Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the 'filter' section the given code returns...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Python List Comprehension Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi there!  Thanks for the tutorial, much appre...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Python List Comprehension Tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's lovely!!!!easy to understand.........than...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Python List Comprehension Tutorial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label  \\\n",
       "0  Thank you for this tutorial , it is simple to ...    0.0   \n",
       "1                                               Good    0.0   \n",
       "2  In the 'filter' section the given code returns...    0.0   \n",
       "3  Hi there!  Thanks for the tutorial, much appre...    0.0   \n",
       "4  It's lovely!!!!easy to understand.........than...    0.0   \n",
       "\n",
       "                          articleName  \n",
       "0  Python List Comprehension Tutorial  \n",
       "1  Python List Comprehension Tutorial  \n",
       "2  Python List Comprehension Tutorial  \n",
       "3  Python List Comprehension Tutorial  \n",
       "4  Python List Comprehension Tutorial  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and validation sets (80:20) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(new_train_frame['comment'], new_train_frame['label'], \\\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing with TF-IDF vectorization - \n",
    "\n",
    "We can write utility functions for this which in turns gives our code a modular approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    # Show extracted features\n",
    "    cache = {}\n",
    "    cache['extractedFeatures'] = vectorizer.get_feature_names()\n",
    "    \n",
    "    return x_train, x_val, vectorizer, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a binary classification problem so the activation function for the output layer of the network will be `sigmoid` as it will give us the class probabilities which we can eventually convert to discrete classes. \n",
    "\n",
    "The following function `get_last_layer_units_and_activation()` is responsible for - \n",
    "* Determining the activation function for the output layer.\n",
    "* Number of units to be present in the output layer. \n",
    "\n",
    "The function can also be extended to multinomial classification tasks as well (where `softmax` activation function is used in the last layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method builds us a feed-forward network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_ngram_model()` function helps to train and validate our shallow network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=3,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Specify the number of classes.\n",
    "    num_classes = 2\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, cache = ngram_vectorize(\n",
    "        train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    loss = 'binary_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    # Tensorboard callback for debugging purpose\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2), tf.keras.callbacks.TensorBoard(log_dir='./logs')]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('zs_mlp_model.h5')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A note on the model configuration choices</b>:\n",
    "* **Dropout layers**: For reducing overfitting. Its value is set 0.2 which means that 20% of the input data will be randomly chosen and excluded from successive activations. This [blog](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/) provides a gentle introduction to dropout regularization. \n",
    "* **Loss function**: We used `binary_crossentropy` as our loss function which is a pretty standard one when it comes to binary classification. It is also convex in nature which in turn helps the optimization algorithm in a faster convergence. \n",
    "* **Adam optimizer**: Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.Whereas in Adam, a learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds.The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating the MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data for fitting it to a feed-forward network - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ((list(X_train), np.array(y_train)), (list(X_valid), np.array(y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a feed-forward networks - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, vectorizer, cache = ngram_vectorize(\n",
    "        X_train, y_train, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extractedFeatures': ['041',\n",
       "  '041 8324',\n",
       "  '10',\n",
       "  '100',\n",
       "  '11',\n",
       "  '14',\n",
       "  '15',\n",
       "  '20',\n",
       "  '2018',\n",
       "  '2019',\n",
       "  '24',\n",
       "  '24 support',\n",
       "  '24 to',\n",
       "  '2d',\n",
       "  '30',\n",
       "  '32',\n",
       "  '44',\n",
       "  '44 800',\n",
       "  '462',\n",
       "  '462 in',\n",
       "  '50',\n",
       "  '64',\n",
       "  '800',\n",
       "  '800 041',\n",
       "  '8324',\n",
       "  '888',\n",
       "  '8884800288',\n",
       "  '8884800288 uk',\n",
       "  '98',\n",
       "  '__init__',\n",
       "  '__init__ self',\n",
       "  'able',\n",
       "  'able to',\n",
       "  'about',\n",
       "  'about how',\n",
       "  'about it',\n",
       "  'about the',\n",
       "  'above',\n",
       "  'access',\n",
       "  'account',\n",
       "  'accounting',\n",
       "  'accuracy',\n",
       "  'achieve',\n",
       "  'across',\n",
       "  'across the',\n",
       "  'active',\n",
       "  'active 24',\n",
       "  'actually',\n",
       "  'add',\n",
       "  'added',\n",
       "  'adding',\n",
       "  'address',\n",
       "  'aditya',\n",
       "  'advance',\n",
       "  'advice',\n",
       "  'advice me',\n",
       "  'after',\n",
       "  'after the',\n",
       "  'again',\n",
       "  'against',\n",
       "  'age',\n",
       "  'ahead',\n",
       "  'ai',\n",
       "  'airlines',\n",
       "  'airlines customer',\n",
       "  'airlines reservations',\n",
       "  'alexisp',\n",
       "  'algorithm',\n",
       "  'algorithms',\n",
       "  'all',\n",
       "  'all thanks',\n",
       "  'all the',\n",
       "  'all these',\n",
       "  'allows',\n",
       "  'almost',\n",
       "  'along',\n",
       "  'alot',\n",
       "  'already',\n",
       "  'already have',\n",
       "  'also',\n",
       "  'also have',\n",
       "  'also want',\n",
       "  'alternative',\n",
       "  'alternative to',\n",
       "  'always',\n",
       "  'am',\n",
       "  'am doing',\n",
       "  'am getting',\n",
       "  'am looking',\n",
       "  'am lost',\n",
       "  'am new',\n",
       "  'am not',\n",
       "  'am stuck',\n",
       "  'am trying',\n",
       "  'am wondering',\n",
       "  'am working',\n",
       "  'amazing',\n",
       "  'amazing tutorial',\n",
       "  'american',\n",
       "  'american airlines',\n",
       "  'an',\n",
       "  'an amazing',\n",
       "  'an argument',\n",
       "  'an error',\n",
       "  'an issue',\n",
       "  'anaconda',\n",
       "  'anaconda3',\n",
       "  'anaconda3 lib',\n",
       "  'analysis',\n",
       "  'analysis on',\n",
       "  'analyzing',\n",
       "  'and',\n",
       "  'and after',\n",
       "  'and am',\n",
       "  'and and',\n",
       "  'and are',\n",
       "  'and being',\n",
       "  'and bolts',\n",
       "  'and code',\n",
       "  'and compute',\n",
       "  'and could',\n",
       "  'and easy',\n",
       "  'and found',\n",
       "  'and get',\n",
       "  'and got',\n",
       "  'and great',\n",
       "  'and have',\n",
       "  'and how',\n",
       "  'and import',\n",
       "  'and in',\n",
       "  'and it',\n",
       "  'and learned',\n",
       "  'and not',\n",
       "  'and offers',\n",
       "  'and plot',\n",
       "  'and queries',\n",
       "  'and see',\n",
       "  'and so',\n",
       "  'and still',\n",
       "  'and store',\n",
       "  'and thanks',\n",
       "  'and the',\n",
       "  'and then',\n",
       "  'and this',\n",
       "  'and very',\n",
       "  'and what',\n",
       "  'and when',\n",
       "  'another',\n",
       "  'antivirus',\n",
       "  'antivirus support',\n",
       "  'any',\n",
       "  'any chance',\n",
       "  'any idea',\n",
       "  'any other',\n",
       "  'any suggestion',\n",
       "  'any way',\n",
       "  'anyone',\n",
       "  'anyone face',\n",
       "  'anytime',\n",
       "  'anyway',\n",
       "  'anywhere',\n",
       "  'api',\n",
       "  'app',\n",
       "  'apple',\n",
       "  'application',\n",
       "  'applied',\n",
       "  'apply',\n",
       "  'apply the',\n",
       "  'appreciate',\n",
       "  'appreciate the',\n",
       "  'appreciate your',\n",
       "  'approach',\n",
       "  'are',\n",
       "  'are not',\n",
       "  'are real',\n",
       "  'are really',\n",
       "  'are the',\n",
       "  'are trying',\n",
       "  'are using',\n",
       "  'are very',\n",
       "  'are you',\n",
       "  'argument',\n",
       "  'around',\n",
       "  'array',\n",
       "  'array with',\n",
       "  'arrays',\n",
       "  'article',\n",
       "  'article and',\n",
       "  'article have',\n",
       "  'article in',\n",
       "  'article is',\n",
       "  'article it',\n",
       "  'article its',\n",
       "  'article thank',\n",
       "  'article very',\n",
       "  'article was',\n",
       "  'articles',\n",
       "  'as',\n",
       "  'as an',\n",
       "  'as input',\n",
       "  'as it',\n",
       "  'as pd',\n",
       "  'as the',\n",
       "  'as we',\n",
       "  'as well',\n",
       "  'as you',\n",
       "  'assuming',\n",
       "  'at',\n",
       "  'at all',\n",
       "  'at american',\n",
       "  'at least',\n",
       "  'at step',\n",
       "  'at this',\n",
       "  'at your',\n",
       "  'attribution',\n",
       "  'author',\n",
       "  'automatically',\n",
       "  'available',\n",
       "  'average',\n",
       "  'average loss',\n",
       "  'avg',\n",
       "  'avinash',\n",
       "  'avoid',\n",
       "  'aware',\n",
       "  'aware of',\n",
       "  'awesome',\n",
       "  'awesome tutorial',\n",
       "  'aws',\n",
       "  'back',\n",
       "  'back stail',\n",
       "  'bars',\n",
       "  'base',\n",
       "  'based',\n",
       "  'based on',\n",
       "  'basic',\n",
       "  'basics',\n",
       "  'basics of',\n",
       "  'batches',\n",
       "  'bayes',\n",
       "  'be',\n",
       "  'be able',\n",
       "  'be back',\n",
       "  'be in',\n",
       "  'be reached',\n",
       "  'be the',\n",
       "  'be used',\n",
       "  'be very',\n",
       "  'because',\n",
       "  'because it',\n",
       "  'because of',\n",
       "  'become',\n",
       "  'been',\n",
       "  'been looking',\n",
       "  'been using',\n",
       "  'been working',\n",
       "  'before',\n",
       "  'beginner',\n",
       "  'beginning',\n",
       "  'being',\n",
       "  'believe',\n",
       "  'believe that',\n",
       "  'belongs',\n",
       "  'belongs to',\n",
       "  'below',\n",
       "  'best',\n",
       "  'best discounts',\n",
       "  'best offers',\n",
       "  'best range',\n",
       "  'best tutorial',\n",
       "  'best way',\n",
       "  'better',\n",
       "  'between',\n",
       "  'bias',\n",
       "  'big',\n",
       "  'binary',\n",
       "  'bit',\n",
       "  'blog',\n",
       "  'blog was',\n",
       "  'blog with',\n",
       "  'boggling',\n",
       "  'boggling information',\n",
       "  'bolts',\n",
       "  'book',\n",
       "  'book cheap',\n",
       "  'booking',\n",
       "  'borough',\n",
       "  'both',\n",
       "  'both the',\n",
       "  'bottom',\n",
       "  'browser',\n",
       "  'build',\n",
       "  'building',\n",
       "  'bunch',\n",
       "  'bunch of',\n",
       "  'business',\n",
       "  'but',\n",
       "  'but can',\n",
       "  'but getting',\n",
       "  'but how',\n",
       "  'but in',\n",
       "  'but it',\n",
       "  'but now',\n",
       "  'but that',\n",
       "  'but the',\n",
       "  'but then',\n",
       "  'but what',\n",
       "  'but when',\n",
       "  'by',\n",
       "  'by day',\n",
       "  'by minmaxscaler',\n",
       "  'by one',\n",
       "  'by the',\n",
       "  'by then',\n",
       "  'by β0',\n",
       "  'cake',\n",
       "  'cake group',\n",
       "  'calculate',\n",
       "  'calculate the',\n",
       "  'calculated',\n",
       "  'call',\n",
       "  'call at',\n",
       "  'call last',\n",
       "  'call us',\n",
       "  'called',\n",
       "  'can',\n",
       "  'can anyone',\n",
       "  'can be',\n",
       "  'can display',\n",
       "  'can do',\n",
       "  'can easily',\n",
       "  'can get',\n",
       "  'can predict',\n",
       "  'can run',\n",
       "  'can say',\n",
       "  'can someone',\n",
       "  'can try',\n",
       "  'can understand',\n",
       "  'can use',\n",
       "  'can visit',\n",
       "  'can we',\n",
       "  'can you',\n",
       "  'canada',\n",
       "  'canada 8884800288',\n",
       "  'cannot',\n",
       "  'cannot open',\n",
       "  'cannot see',\n",
       "  'canon',\n",
       "  'canon printer',\n",
       "  'care',\n",
       "  'care number',\n",
       "  'care toll',\n",
       "  'case',\n",
       "  'case of',\n",
       "  'categorical',\n",
       "  'categorical variables',\n",
       "  'cell',\n",
       "  'center',\n",
       "  'chain',\n",
       "  'chains',\n",
       "  'chance',\n",
       "  'change',\n",
       "  'change the',\n",
       "  'changed',\n",
       "  'changes',\n",
       "  'changes the',\n",
       "  'character',\n",
       "  'cheap',\n",
       "  'cheap flight',\n",
       "  'check',\n",
       "  'check_array',\n",
       "  'check_array context',\n",
       "  'cheers',\n",
       "  'chennai',\n",
       "  'choose',\n",
       "  'chunk',\n",
       "  'clarify',\n",
       "  'class',\n",
       "  'classification',\n",
       "  'classifier',\n",
       "  'clear',\n",
       "  'clear and',\n",
       "  'click',\n",
       "  'click here',\n",
       "  'client',\n",
       "  'close',\n",
       "  'cloud',\n",
       "  'cltv',\n",
       "  'cltv in',\n",
       "  'cluster',\n",
       "  'clustering',\n",
       "  'clustering is',\n",
       "  'clusters',\n",
       "  'cnn',\n",
       "  'cnn and',\n",
       "  'code',\n",
       "  'code above',\n",
       "  'code and',\n",
       "  'code for',\n",
       "  'code in',\n",
       "  'code is',\n",
       "  'code it',\n",
       "  'code should',\n",
       "  'code to',\n",
       "  'code you',\n",
       "  'codes',\n",
       "  'coding',\n",
       "  'color',\n",
       "  'color of',\n",
       "  'column',\n",
       "  'column in',\n",
       "  'column so',\n",
       "  'columns',\n",
       "  'columns for',\n",
       "  'com',\n",
       "  'come',\n",
       "  'comes',\n",
       "  'comes to',\n",
       "  'command',\n",
       "  'community',\n",
       "  'company',\n",
       "  'compare',\n",
       "  'compared',\n",
       "  'comprehension',\n",
       "  'comprehension of',\n",
       "  'comprehensive',\n",
       "  'compute',\n",
       "  'compute the',\n",
       "  'compute_coherence_values',\n",
       "  'computer',\n",
       "  'concept',\n",
       "  'condition',\n",
       "  'confuse',\n",
       "  'confused',\n",
       "  'connect',\n",
       "  'connecting',\n",
       "  'connection',\n",
       "  'consider',\n",
       "  'considered',\n",
       "  'considering',\n",
       "  'consistent',\n",
       "  'contact',\n",
       "  'content',\n",
       "  'context',\n",
       "  'context valueerror',\n",
       "  'continue',\n",
       "  'continuous',\n",
       "  'contrib',\n",
       "  'contrib rnn',\n",
       "  'convert',\n",
       "  'convert the',\n",
       "  'cookie',\n",
       "  'cool',\n",
       "  'copy',\n",
       "  'core',\n",
       "  'core of',\n",
       "  'correct',\n",
       "  'correctly',\n",
       "  'could',\n",
       "  'could be',\n",
       "  'could not',\n",
       "  'could provide',\n",
       "  'could share',\n",
       "  'could someone',\n",
       "  'could you',\n",
       "  'couldn',\n",
       "  'course',\n",
       "  'courses',\n",
       "  'cpu',\n",
       "  'create',\n",
       "  'created',\n",
       "  'creating',\n",
       "  'creating style',\n",
       "  'csv',\n",
       "  'csv file',\n",
       "  'csv however',\n",
       "  'curious',\n",
       "  'current',\n",
       "  'currently',\n",
       "  'curve',\n",
       "  'custom',\n",
       "  'customer',\n",
       "  'customer care',\n",
       "  'customer service',\n",
       "  'customer support',\n",
       "  'dan',\n",
       "  'dash',\n",
       "  'data',\n",
       "  'data frame',\n",
       "  'data into',\n",
       "  'data like',\n",
       "  'data please',\n",
       "  'data science',\n",
       "  'data scientists',\n",
       "  'data set',\n",
       "  'data test',\n",
       "  'data to',\n",
       "  'data14',\n",
       "  'data14 could',\n",
       "  'data14 month',\n",
       "  'database',\n",
       "  'datacamp',\n",
       "  'dataframe',\n",
       "  'datalab',\n",
       "  'dataset',\n",
       "  'dataset for',\n",
       "  'dataset which',\n",
       "  'datasets',\n",
       "  'date',\n",
       "  'date time',\n",
       "  'day',\n",
       "  'day by',\n",
       "  'days',\n",
       "  'dealing',\n",
       "  'dealing with',\n",
       "  'deals',\n",
       "  'deals on',\n",
       "  'dear',\n",
       "  'dear phillips',\n",
       "  'decide',\n",
       "  'decision',\n",
       "  'decrease',\n",
       "  'deep',\n",
       "  'deep learning',\n",
       "  'def',\n",
       "  'def __init__',\n",
       "  'def generate_signals',\n",
       "  'default',\n",
       "  'defined',\n",
       "  'definitely',\n",
       "  'delta',\n",
       "  'delta airlines',\n",
       "  'described',\n",
       "  'description',\n",
       "  'detail',\n",
       "  'detailed',\n",
       "  'detailed description',\n",
       "  'details',\n",
       "  'df',\n",
       "  'df pd',\n",
       "  'dial',\n",
       "  'dial american',\n",
       "  'dictionary',\n",
       "  'did',\n",
       "  'did you',\n",
       "  'didn',\n",
       "  'difference',\n",
       "  'different',\n",
       "  'difficulty',\n",
       "  'direction',\n",
       "  'directory',\n",
       "  'discounts',\n",
       "  'discounts and',\n",
       "  'display',\n",
       "  'display the',\n",
       "  'distance',\n",
       "  'distribution',\n",
       "  'do',\n",
       "  'do get',\n",
       "  'do it',\n",
       "  'do not',\n",
       "  'do that',\n",
       "  'do the',\n",
       "  'do they',\n",
       "  'do this',\n",
       "  'do we',\n",
       "  'do with',\n",
       "  'do you',\n",
       "  'document',\n",
       "  'documentation',\n",
       "  'does',\n",
       "  'does it',\n",
       "  'does not',\n",
       "  'doesn',\n",
       "  'doesn have',\n",
       "  'doesn look',\n",
       "  'doesn work',\n",
       "  'doing',\n",
       "  'doing the',\n",
       "  'don',\n",
       "  'don know',\n",
       "  'don understand',\n",
       "  'done',\n",
       "  'dont',\n",
       "  'dont know',\n",
       "  'doubt',\n",
       "  'down',\n",
       "  'download',\n",
       "  'downloading',\n",
       "  'dtype',\n",
       "  'due',\n",
       "  'due to',\n",
       "  'during',\n",
       "  'each',\n",
       "  'each sample',\n",
       "  'easily',\n",
       "  'easy',\n",
       "  'easy to',\n",
       "  'eat',\n",
       "  'eat cake',\n",
       "  'eat tcake',\n",
       "  'eda',\n",
       "  'education',\n",
       "  'effective',\n",
       "  'effort',\n",
       "  'either',\n",
       "  'element',\n",
       "  'elements',\n",
       "  'elif',\n",
       "  'else',\n",
       "  'else statements',\n",
       "  'email',\n",
       "  'encoded',\n",
       "  'encoded what',\n",
       "  'encoding',\n",
       "  'end',\n",
       "  'engine',\n",
       "  'enjoy',\n",
       "  'enjoyed',\n",
       "  'enjoying',\n",
       "  'enough',\n",
       "  'entire',\n",
       "  'environment',\n",
       "  'epson',\n",
       "  'epson printer',\n",
       "  'error',\n",
       "  'error at',\n",
       "  'error code',\n",
       "  'error evaluation',\n",
       "  'error for',\n",
       "  'error in',\n",
       "  'error message',\n",
       "  'error while',\n",
       "  'errors',\n",
       "  'especially',\n",
       "  'estimator',\n",
       "  'etc',\n",
       "  'etc but',\n",
       "  'euclidean',\n",
       "  'euclidean distance',\n",
       "  'evaluation',\n",
       "  'even',\n",
       "  'even though',\n",
       "  'ever',\n",
       "  'every',\n",
       "  'everything',\n",
       "  'exactly',\n",
       "  'example',\n",
       "  'example in',\n",
       "  'example of',\n",
       "  'examples',\n",
       "  'excel',\n",
       "  'excel file',\n",
       "  'excel files',\n",
       "  'excel tutorial',\n",
       "  'excellent',\n",
       "  'excellent article',\n",
       "  'exciting',\n",
       "  'exclusive',\n",
       "  'executing',\n",
       "  'exiting',\n",
       "  'expect',\n",
       "  'expected',\n",
       "  'experience',\n",
       "  'experienced',\n",
       "  'experts',\n",
       "  'experts will',\n",
       "  'explain',\n",
       "  'explain me',\n",
       "  'explain the',\n",
       "  'explain what',\n",
       "  'explained',\n",
       "  'explaining',\n",
       "  'explanation',\n",
       "  'explanation that',\n",
       "  'explore',\n",
       "  'expressions',\n",
       "  'extremely',\n",
       "  'face',\n",
       "  'face printer',\n",
       "  'factor',\n",
       "  'failed',\n",
       "  'failed to',\n",
       "  'failed with',\n",
       "  'failure',\n",
       "  'false',\n",
       "  'familiar',\n",
       "  'familiar with',\n",
       "  'family',\n",
       "  'fantastic',\n",
       "  'far',\n",
       "  'fashion',\n",
       "  'fast',\n",
       "  'feature',\n",
       "  'features',\n",
       "  'features like',\n",
       "  'few',\n",
       "  'fftrees',\n",
       "  'field',\n",
       "  'figure',\n",
       "  'file',\n",
       "  'file how',\n",
       "  'files',\n",
       "  'filter',\n",
       "  'finance',\n",
       "  'find',\n",
       "  'find problem',\n",
       "  'findall',\n",
       "  'finished',\n",
       "  'finished predictions',\n",
       "  'first',\n",
       "  'first of',\n",
       "  'firstly',\n",
       "  'fit',\n",
       "  'fix',\n",
       "  'fix it',\n",
       "  'fixed',\n",
       "  'flight',\n",
       "  'flight tickets',\n",
       "  'flights',\n",
       "  'float',\n",
       "  'follow',\n",
       "  'follow you',\n",
       "  'followed',\n",
       "  'following',\n",
       "  'following error',\n",
       "  'following line',\n",
       "  'following message',\n",
       "  'for',\n",
       "  'for all',\n",
       "  'for any',\n",
       "  'for data',\n",
       "  'for each',\n",
       "  'for example',\n",
       "  'for help',\n",
       "  'for in',\n",
       "  'for li',\n",
       "  'for loop',\n",
       "  'for me',\n",
       "  'for more',\n",
       "  'for multiple',\n",
       "  'for my',\n",
       "  'for posting',\n",
       "  'for putting',\n",
       "  'for sharing',\n",
       "  'for some',\n",
       "  'for such',\n",
       "  'for that',\n",
       "  'for the',\n",
       "  'for this',\n",
       "  'for us',\n",
       "  'for windows',\n",
       "  'for your',\n",
       "  'form',\n",
       "  'form experts',\n",
       "  'format',\n",
       "  'forward',\n",
       "  'found',\n",
       "  'found array',\n",
       "  'found this',\n",
       "  'found your',\n",
       "  'frame',\n",
       "  'framework',\n",
       "  'free',\n",
       "  'free number',\n",
       "  'free technical',\n",
       "  'friends',\n",
       "  'from',\n",
       "  'from https',\n",
       "  'from that',\n",
       "  'from the',\n",
       "  'full',\n",
       "  'function',\n",
       "  'function is',\n",
       "  'function of',\n",
       "  'functions',\n",
       "  'further',\n",
       "  'future',\n",
       "  'game',\n",
       "  'games',\n",
       "  'gateway',\n",
       "  'gateway uae',\n",
       "  'gaussiannb',\n",
       "  'gcp',\n",
       "  'gender',\n",
       "  'generate',\n",
       "  'generate_signals',\n",
       "  'generate_signals self',\n",
       "  'generation',\n",
       "  'gensim',\n",
       "  'geocode',\n",
       "  'geocode failed',\n",
       "  'geom_point',\n",
       "  'get',\n",
       "  'get an',\n",
       "  'get best',\n",
       "  'get free',\n",
       "  'get in',\n",
       "  'get into',\n",
       "  'get it',\n",
       "  'get more',\n",
       "  'get the',\n",
       "  'get this',\n",
       "  'get what',\n",
       "  'get your',\n",
       "  'get_map',\n",
       "  'get_map function',\n",
       "  'getting',\n",
       "  'getting the',\n",
       "  'getting this',\n",
       "  'github',\n",
       "  'github com',\n",
       "  'give',\n",
       "  'give me',\n",
       "  'give you',\n",
       "  'given',\n",
       "  'gives',\n",
       "  'gives me',\n",
       "  'giving',\n",
       "  'globe',\n",
       "  'go',\n",
       "  'goal',\n",
       "  'goal is',\n",
       "  'goes',\n",
       "  'going',\n",
       "  'going to',\n",
       "  'gone',\n",
       "  'gone through',\n",
       "  'good',\n",
       "  'good article',\n",
       "  'google',\n",
       "  'google cloud',\n",
       "  'got',\n",
       "  'got an',\n",
       "  'gpu',\n",
       "  'graph',\n",
       "  'graphs',\n",
       "  'great',\n",
       "  'great article',\n",
       "  'great job',\n",
       "  'great post',\n",
       "  'great thanks',\n",
       "  'great tutorial',\n",
       "  'group',\n",
       "  'groupby',\n",
       "  'groups',\n",
       "  'groups and',\n",
       "  'guess',\n",
       "  'guessing',\n",
       "  'guidance',\n",
       "  'guide',\n",
       "  'guide me',\n",
       "  'guys',\n",
       "  'had',\n",
       "  'happened',\n",
       "  'happening',\n",
       "  'hard',\n",
       "  'hard time',\n",
       "  'hard to',\n",
       "  'has',\n",
       "  'has been',\n",
       "  'has to',\n",
       "  'have',\n",
       "  'have an',\n",
       "  'have any',\n",
       "  'have been',\n",
       "  'have found',\n",
       "  'have gone',\n",
       "  'have got',\n",
       "  'have it',\n",
       "  'have one',\n",
       "  'have problem',\n",
       "  'have question',\n",
       "  'have some',\n",
       "  'have the',\n",
       "  'have three',\n",
       "  'have to',\n",
       "  'have used',\n",
       "  'have you',\n",
       "  'haven',\n",
       "  'haven package',\n",
       "  'having',\n",
       "  'he',\n",
       "  'heart',\n",
       "  'hello',\n",
       "  'hello am',\n",
       "  'hello thanks',\n",
       "  'help',\n",
       "  'help me',\n",
       "  'help of',\n",
       "  'help to',\n",
       "  'help us',\n",
       "  'help you',\n",
       "  'helped',\n",
       "  'helped me',\n",
       "  'helpful',\n",
       "  'helpful for',\n",
       "  'helpful to',\n",
       "  'helpful with',\n",
       "  'here',\n",
       "  'here https',\n",
       "  'here to',\n",
       "  'here we',\n",
       "  'hey',\n",
       "  'hey there',\n",
       "  'hi',\n",
       "  'hi alexisp',\n",
       "  'hi avinash',\n",
       "  'hi have',\n",
       "  'hi how',\n",
       "  'hi sejal',\n",
       "  'hi thank',\n",
       "  'hi there',\n",
       "  'hi thushan',\n",
       "  'hierarchies',\n",
       "  'high',\n",
       "  'highest',\n",
       "  'highly',\n",
       "  'hii',\n",
       "  'home',\n",
       "  'hope',\n",
       "  'hope you',\n",
       "  'hopefully',\n",
       "  'host',\n",
       "  'hot',\n",
       "  'hot encoding',\n",
       "  'hours',\n",
       "  'how',\n",
       "  'how can',\n",
       "  'how could',\n",
       "  'how did',\n",
       "  'how do',\n",
       "  'how should',\n",
       "  'how to',\n",
       "  'how would',\n",
       "  'how you',\n",
       "  'however',\n",
       "  'however the',\n",
       "  'hp',\n",
       "  'hp printer',\n",
       "  'href',\n",
       "  'href https',\n",
       "  'html',\n",
       "  'https',\n",
       "  'https github',\n",
       "  'https www',\n",
       "  'hugo',\n",
       "  'icecream',\n",
       "  'id',\n",
       "  'ide',\n",
       "  'idea',\n",
       "  'identical',\n",
       "  'identified',\n",
       "  'identify',\n",
       "  'identify the',\n",
       "  'ie',\n",
       "  'if',\n",
       "  'if anyone',\n",
       "  'if elif',\n",
       "  'if it',\n",
       "  'if the',\n",
       "  'if use',\n",
       "  'if we',\n",
       "  'if you',\n",
       "  'if your',\n",
       "  'im',\n",
       "  'image',\n",
       "  'images',\n",
       "  'implement',\n",
       "  'implementation',\n",
       "  'implemented',\n",
       "  'import',\n",
       "  'import pandas',\n",
       "  'important',\n",
       "  'impressed',\n",
       "  'improve',\n",
       "  'in',\n",
       "  'in advance',\n",
       "  'in case',\n",
       "  'in check_array',\n",
       "  'in chennai',\n",
       "  'in data',\n",
       "  'in first',\n",
       "  'in logistic',\n",
       "  'in module',\n",
       "  'in my',\n",
       "  'in new',\n",
       "  'in obtaining',\n",
       "  'in one',\n",
       "  'in python',\n",
       "  'in range',\n",
       "  'in short',\n",
       "  'in simple',\n",
       "  'in spyder',\n",
       "  'in tensorflow',\n",
       "  'in that',\n",
       "  'in the',\n",
       "  'in this',\n",
       "  'in touch',\n",
       "  'in which',\n",
       "  'in your',\n",
       "  ...]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\programfiles\\anaconda\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From d:\\programfiles\\anaconda\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 486 samples, validate on 122 samples\n",
      "WARNING:tensorflow:From d:\\programfiles\\anaconda\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 0.7004 - acc: 0.4671 - val_loss: 0.6845 - val_acc: 0.7787\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 0.6849 - acc: 0.5947 - val_loss: 0.6741 - val_acc: 0.7787\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 0.6673 - acc: 0.7140 - val_loss: 0.6638 - val_acc: 0.7787\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.6535 - acc: 0.7387 - val_loss: 0.6519 - val_acc: 0.7787\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.6384 - acc: 0.7942 - val_loss: 0.6382 - val_acc: 0.7787\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.6183 - acc: 0.8251 - val_loss: 0.6227 - val_acc: 0.7787\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.6031 - acc: 0.8272 - val_loss: 0.6050 - val_acc: 0.7787\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 0.5789 - acc: 0.8230 - val_loss: 0.5853 - val_acc: 0.7787\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 0.5636 - acc: 0.8251 - val_loss: 0.5644 - val_acc: 0.7787\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 0.5430 - acc: 0.8210 - val_loss: 0.5436 - val_acc: 0.7787\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 0.5118 - acc: 0.8272 - val_loss: 0.5229 - val_acc: 0.7787\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 0.4918 - acc: 0.8272 - val_loss: 0.5039 - val_acc: 0.7787\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 0.4750 - acc: 0.8272 - val_loss: 0.4874 - val_acc: 0.7787\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 0.4716 - acc: 0.8272 - val_loss: 0.4743 - val_acc: 0.7787\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 0.4397 - acc: 0.8272 - val_loss: 0.4647 - val_acc: 0.7787\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 0.4413 - acc: 0.8272 - val_loss: 0.4576 - val_acc: 0.7787\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 0.4142 - acc: 0.8272 - val_loss: 0.4519 - val_acc: 0.7787\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 0.4423 - acc: 0.8272 - val_loss: 0.4464 - val_acc: 0.7787\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 0.4030 - acc: 0.8292 - val_loss: 0.4410 - val_acc: 0.7787\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 0.4048 - acc: 0.8272 - val_loss: 0.4357 - val_acc: 0.7787\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 0.3963 - acc: 0.8272 - val_loss: 0.4297 - val_acc: 0.7787\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 0.4052 - acc: 0.8272 - val_loss: 0.4237 - val_acc: 0.7787\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 0.3759 - acc: 0.8272 - val_loss: 0.4174 - val_acc: 0.7787\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 0.3685 - acc: 0.8272 - val_loss: 0.4117 - val_acc: 0.7787\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 0.3946 - acc: 0.8272 - val_loss: 0.4063 - val_acc: 0.7787\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 0.3674 - acc: 0.8272 - val_loss: 0.4005 - val_acc: 0.7787\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 0.3663 - acc: 0.8272 - val_loss: 0.3953 - val_acc: 0.7787\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 0.3517 - acc: 0.8272 - val_loss: 0.3900 - val_acc: 0.7787\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 0.3366 - acc: 0.8272 - val_loss: 0.3848 - val_acc: 0.7787\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 0.3500 - acc: 0.8272 - val_loss: 0.3796 - val_acc: 0.7787\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 0.3274 - acc: 0.8292 - val_loss: 0.3750 - val_acc: 0.7787\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 0.3604 - acc: 0.8272 - val_loss: 0.3706 - val_acc: 0.7787\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 0.3112 - acc: 0.8313 - val_loss: 0.3661 - val_acc: 0.7787\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 0.3211 - acc: 0.8272 - val_loss: 0.3619 - val_acc: 0.7787\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 0.3075 - acc: 0.8313 - val_loss: 0.3577 - val_acc: 0.7787\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 0.3006 - acc: 0.8354 - val_loss: 0.3536 - val_acc: 0.7787\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.3091 - acc: 0.8313 - val_loss: 0.3498 - val_acc: 0.7787\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.2894 - acc: 0.8395 - val_loss: 0.3458 - val_acc: 0.7787\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.2875 - acc: 0.8354 - val_loss: 0.3423 - val_acc: 0.8197\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.2812 - acc: 0.8374 - val_loss: 0.3383 - val_acc: 0.8361\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.2789 - acc: 0.8498 - val_loss: 0.3339 - val_acc: 0.8689\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.2600 - acc: 0.8601 - val_loss: 0.3300 - val_acc: 0.8689\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.2610 - acc: 0.8560 - val_loss: 0.3261 - val_acc: 0.8770\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.2607 - acc: 0.8601 - val_loss: 0.3214 - val_acc: 0.8770\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.2515 - acc: 0.8786 - val_loss: 0.3166 - val_acc: 0.8770\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.2578 - acc: 0.8745 - val_loss: 0.3114 - val_acc: 0.8852\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.2413 - acc: 0.8909 - val_loss: 0.3050 - val_acc: 0.8852\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.2406 - acc: 0.8992 - val_loss: 0.2991 - val_acc: 0.8852\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.2358 - acc: 0.9012 - val_loss: 0.2941 - val_acc: 0.8852\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.2313 - acc: 0.8951 - val_loss: 0.2889 - val_acc: 0.8852\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.2377 - acc: 0.8992 - val_loss: 0.2851 - val_acc: 0.8852\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.2151 - acc: 0.9033 - val_loss: 0.2807 - val_acc: 0.8852\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.2159 - acc: 0.9095 - val_loss: 0.2758 - val_acc: 0.8934\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.2082 - acc: 0.9177 - val_loss: 0.2720 - val_acc: 0.8934\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.1954 - acc: 0.9342 - val_loss: 0.2707 - val_acc: 0.8934\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.1996 - acc: 0.9218 - val_loss: 0.2707 - val_acc: 0.8934\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.1841 - acc: 0.9280 - val_loss: 0.2686 - val_acc: 0.8934\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.1875 - acc: 0.9300 - val_loss: 0.2668 - val_acc: 0.8934\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.1960 - acc: 0.9342 - val_loss: 0.2624 - val_acc: 0.8934\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.1990 - acc: 0.9280 - val_loss: 0.2577 - val_acc: 0.8852\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.1689 - acc: 0.9403 - val_loss: 0.2555 - val_acc: 0.8934\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.1814 - acc: 0.9239 - val_loss: 0.2537 - val_acc: 0.8934\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.1899 - acc: 0.9362 - val_loss: 0.2523 - val_acc: 0.8934\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.1584 - acc: 0.9403 - val_loss: 0.2523 - val_acc: 0.8934\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.1679 - acc: 0.9424 - val_loss: 0.2537 - val_acc: 0.8934\n",
      "Validation accuracy: 0.8934426307678223, loss: 0.2537039816379547\n"
     ]
    }
   ],
   "source": [
    "h = train_ngram_model(data,\\\n",
    "                      learning_rate=1e-3,\\\n",
    "                      epochs=1000,\\\n",
    "                      batch_size=128,\\\n",
    "                      layers=3,\\\n",
    "                      units=64,\\\n",
    "                      dropout_rate=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 486 samples, validate on 122 samples\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 0.7042 - acc: 0.4506 - val_loss: 0.6850 - val_acc: 0.7787\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 0.6872 - acc: 0.5885 - val_loss: 0.6732 - val_acc: 0.7787\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 0.6673 - acc: 0.6996 - val_loss: 0.6613 - val_acc: 0.7787\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 0.6491 - acc: 0.7860 - val_loss: 0.6485 - val_acc: 0.7787\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 0.6318 - acc: 0.8025 - val_loss: 0.6341 - val_acc: 0.7787\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 0.6121 - acc: 0.8148 - val_loss: 0.6182 - val_acc: 0.7787\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 0.5949 - acc: 0.8272 - val_loss: 0.6006 - val_acc: 0.7787\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 0.5825 - acc: 0.8230 - val_loss: 0.5814 - val_acc: 0.7787\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 0.5648 - acc: 0.8251 - val_loss: 0.5615 - val_acc: 0.7787\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 0.5230 - acc: 0.8272 - val_loss: 0.5407 - val_acc: 0.7787\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 0.5140 - acc: 0.8272 - val_loss: 0.5210 - val_acc: 0.7787\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 0.4951 - acc: 0.8272 - val_loss: 0.5029 - val_acc: 0.7787\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 0.4678 - acc: 0.8272 - val_loss: 0.4871 - val_acc: 0.7787\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 0.4803 - acc: 0.8272 - val_loss: 0.4745 - val_acc: 0.7787\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 0.4437 - acc: 0.8272 - val_loss: 0.4651 - val_acc: 0.7787\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 0.4415 - acc: 0.8272 - val_loss: 0.4576 - val_acc: 0.7787\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 0.4405 - acc: 0.8272 - val_loss: 0.4517 - val_acc: 0.7787\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 0.4053 - acc: 0.8272 - val_loss: 0.4464 - val_acc: 0.7787\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 0.4075 - acc: 0.8272 - val_loss: 0.4411 - val_acc: 0.7787\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 0.4054 - acc: 0.8251 - val_loss: 0.4358 - val_acc: 0.7787\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 0.4228 - acc: 0.8272 - val_loss: 0.4303 - val_acc: 0.7787\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 0.3922 - acc: 0.8272 - val_loss: 0.4247 - val_acc: 0.7787\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 0.3967 - acc: 0.8272 - val_loss: 0.4190 - val_acc: 0.7787\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 0.3978 - acc: 0.8272 - val_loss: 0.4129 - val_acc: 0.7787\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 0.3848 - acc: 0.8292 - val_loss: 0.4069 - val_acc: 0.7787\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 0.3707 - acc: 0.8292 - val_loss: 0.4008 - val_acc: 0.7787\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 0.3787 - acc: 0.8292 - val_loss: 0.3953 - val_acc: 0.7787\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 0.3592 - acc: 0.8292 - val_loss: 0.3904 - val_acc: 0.7787\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 0.3552 - acc: 0.8272 - val_loss: 0.3862 - val_acc: 0.7787\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 0.3383 - acc: 0.8272 - val_loss: 0.3818 - val_acc: 0.7787\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 0.3399 - acc: 0.8313 - val_loss: 0.3782 - val_acc: 0.7787\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 0.3526 - acc: 0.8272 - val_loss: 0.3748 - val_acc: 0.7787\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 0.3370 - acc: 0.8292 - val_loss: 0.3711 - val_acc: 0.7787\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 0.3128 - acc: 0.8292 - val_loss: 0.3669 - val_acc: 0.7787\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 0.3185 - acc: 0.8292 - val_loss: 0.3629 - val_acc: 0.7787\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 0.3220 - acc: 0.8313 - val_loss: 0.3589 - val_acc: 0.7787\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.2972 - acc: 0.8313 - val_loss: 0.3549 - val_acc: 0.7787\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.2982 - acc: 0.8395 - val_loss: 0.3511 - val_acc: 0.7787\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.2929 - acc: 0.8333 - val_loss: 0.3475 - val_acc: 0.7787\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.2806 - acc: 0.8498 - val_loss: 0.3446 - val_acc: 0.8033\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.2915 - acc: 0.8395 - val_loss: 0.3416 - val_acc: 0.8279\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.2787 - acc: 0.8477 - val_loss: 0.3389 - val_acc: 0.8525\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.2531 - acc: 0.8601 - val_loss: 0.3361 - val_acc: 0.8607\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.2825 - acc: 0.8498 - val_loss: 0.3331 - val_acc: 0.8607\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.2677 - acc: 0.8663 - val_loss: 0.3291 - val_acc: 0.8770\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.2623 - acc: 0.8601 - val_loss: 0.3252 - val_acc: 0.8852\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.2794 - acc: 0.8580 - val_loss: 0.3204 - val_acc: 0.8852\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.2524 - acc: 0.8765 - val_loss: 0.3159 - val_acc: 0.8852\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.2516 - acc: 0.8848 - val_loss: 0.3117 - val_acc: 0.8852\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.2600 - acc: 0.8765 - val_loss: 0.3069 - val_acc: 0.8852\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.2509 - acc: 0.8971 - val_loss: 0.3011 - val_acc: 0.8852\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.2395 - acc: 0.8992 - val_loss: 0.2965 - val_acc: 0.8852\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.2346 - acc: 0.8909 - val_loss: 0.2936 - val_acc: 0.8852\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.2299 - acc: 0.8909 - val_loss: 0.2906 - val_acc: 0.8852\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.2299 - acc: 0.9095 - val_loss: 0.2883 - val_acc: 0.8852\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.2268 - acc: 0.8889 - val_loss: 0.2840 - val_acc: 0.8852\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.1975 - acc: 0.9198 - val_loss: 0.2785 - val_acc: 0.9016\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.2136 - acc: 0.9156 - val_loss: 0.2726 - val_acc: 0.9016\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.2034 - acc: 0.9259 - val_loss: 0.2706 - val_acc: 0.9016\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.2030 - acc: 0.9136 - val_loss: 0.2698 - val_acc: 0.9016\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.2065 - acc: 0.9280 - val_loss: 0.2695 - val_acc: 0.9016\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.1699 - acc: 0.9403 - val_loss: 0.2710 - val_acc: 0.9016\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.1754 - acc: 0.9259 - val_loss: 0.2687 - val_acc: 0.9016\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.1714 - acc: 0.9342 - val_loss: 0.2675 - val_acc: 0.9016\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.1471 - acc: 0.9486 - val_loss: 0.2611 - val_acc: 0.9098\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 0.1835 - acc: 0.9342 - val_loss: 0.2562 - val_acc: 0.9098\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 0.1614 - acc: 0.9424 - val_loss: 0.2527 - val_acc: 0.9098\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 0.1602 - acc: 0.9383 - val_loss: 0.2506 - val_acc: 0.9098\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 0.1541 - acc: 0.9383 - val_loss: 0.2500 - val_acc: 0.9098\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 0.1524 - acc: 0.9321 - val_loss: 0.2517 - val_acc: 0.9098\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 0.1749 - acc: 0.9259 - val_loss: 0.2577 - val_acc: 0.9098\n",
      "Validation accuracy: 0.9098360538482666, loss: 0.25773370265960693\n"
     ]
    }
   ],
   "source": [
    "h = train_ngram_model(data,\\\n",
    "                      learning_rate=1e-3,\\\n",
    "                      epochs=1000,\\\n",
    "                      batch_size=128,\\\n",
    "                      layers=3,\\\n",
    "                      units=64,\\\n",
    "                      dropout_rate=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The validation accuracy has increased to a good extent with a shallow net. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting and summarization of loss and accuracy - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss and accuracy.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    " \n",
    "# Plot\n",
    "N = len(h['loss'])\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), h[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), h[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), h[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), h[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the classification report with the help of the serialized model\n",
    "\n",
    "We have the model serialized in a .h5 file. We load it up and use it for evaluation and for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "        model = load_model('zs_mlp_model.h5') # We serialized the weights of the feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `CustomObjectScope` to load up the `model.h5` file since there were compatibility issues. The `CustomObjectScope` needs an instance to be passed hence the use of the `gloroth_uniform` initializer. It does not have any other special purpose. \n",
    "\n",
    "Stackoverflow thread referred for this: https://stackoverflow.com/questions/53183865/unknown-initializer-glorotuniform-when-loading-keras-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94        95\n",
      "         1.0       0.94      0.56      0.70        27\n",
      "\n",
      "    accuracy                           0.89       122\n",
      "   macro avg       0.91      0.77      0.82       122\n",
      "weighted avg       0.90      0.89      0.88       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "_, validation_vectors, cache = ngram_vectorize(\n",
    "        list(X_train), np.array(y_train), list(X_valid))\n",
    "valid_preds = model.predict_classes(validation_vectors)\n",
    "print(classification_report(y_valid, valid_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MLP model beats the baseline when it comes to identifying the minority classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this model to make predictions on the actual test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the architecture of the model that we used - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 2564)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                164160    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 168,385\n",
      "Trainable params: 168,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = vectorizer.transform(np.array(['I have recently opened a restaurant']))\n",
    "model.predict_classes(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\programfiles\\anaconda\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programfiles\\anaconda\\envs\\nlp\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1959\u001b[0m                 raise InvocationException(\n\u001b[1;32m-> 1960\u001b[1;33m                     'GraphViz\\'s executables not found')\n\u001b[0m\u001b[0;32m   1961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvocationException\u001b[0m: GraphViz's executables not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c3953ca0b6c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\programfiles\\anaconda\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    146\u001b[0m           \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m   \"\"\"\n\u001b[1;32m--> 148\u001b[1;33m   \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programfiles\\anaconda\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     69\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m   \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m   \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programfiles\\anaconda\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# pydot raises a generic Exception here,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# so no specific class can be caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     50\u001b[0m                       ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,to_file='model.png',show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first node denotes the inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_submission = pd.read_csv('../dataset/test.csv', encoding = \"ISO-8859-1\")\n",
    "submission = pd.DataFrame()\n",
    "submission['Index'] = test_for_submission['Index']\n",
    "submission['Patient_Tag'] = test_preds\n",
    "\n",
    "submission.to_csv('submission03.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 submission03.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further considerations - \n",
    "\n",
    "* Hyperparameter tuning.\n",
    "* Prediction ensembles. \n",
    "* Build a simple REST API of the shallow net so that it can be deployed in production. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
